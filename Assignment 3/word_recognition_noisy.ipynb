{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from hmmlearn import hmm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the code I used to do Word Detection on NOISY data after Endpoint Detection.\n",
    "NOISE has been added to all the dataset and that noisy data has been used to detect the words.\n",
    "All the endpoint detection data is saved to a folder called as Commands_Dataset.\n",
    "And from there the data is loaded. The dataset folder will not be included in the submission\n",
    "due to size constraints.\n",
    "\n",
    "The following code uses Gaussian HMM with number of states = 5\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(filepath):\n",
    "    y, sr = librosa.load(filepath)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc2 = librosa.feature.delta(mfcc,order=1)\n",
    "    mfcc3 = librosa.feature.delta(mfcc,order=2)\n",
    "    final_mfcc = np.concatenate((mfcc, mfcc2, mfcc3), axis=0)\n",
    "    return final_mfcc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGoogleDataSet(dir):\n",
    "    dataset = {}\n",
    "    for f in os.listdir(dir):#os.listdir(dir)\n",
    "        for w in os.listdir(dir + f):\n",
    "            feature = extract_mfcc_features(dir+ f + '/' + w)\n",
    "            label = f\n",
    "            if label not in dataset.keys():\n",
    "                dataset[label] = []\n",
    "                dataset[label].append(feature)\n",
    "            else:\n",
    "                exist_feat = dataset[label]\n",
    "                exist_feat.append(feature)\n",
    "                dataset[label] = exist_feat\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'w+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GMMHMM(dataset):\n",
    "    GMMHMM_Models = {}\n",
    "    states_num = 6\n",
    "    for label in dataset.keys():\n",
    "        model = hmm.GaussianHMM(n_components=states_num, \\\n",
    "                           covariance_type='full', n_iter=100)\n",
    "        trainData = dataset[label]\n",
    "        length = np.zeros([len(trainData), ], dtype=np.int)\n",
    "        for m in range(len(trainData)):\n",
    "            length[m] = trainData[m].shape[0]\n",
    "        trainData = np.vstack(trainData)\n",
    "        model.fit(trainData, lengths=length)  # get optimal parameters\n",
    "        GMMHMM_Models[label] = model\n",
    "    return GMMHMM_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"Commands_Dataset/train_noisy/\"\n",
    "trainDataSet = buildGoogleDataSet(trainDir)\n",
    "print(\"Finish prepare the training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataSet = load_obj('trainDataSet')\n",
    "print(len(trainDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmmModels = train_GMMHMM(trainDataSet)\n",
    "print(\"Finish training of the GMM_HMM models for 10 command words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(trainDataSet, 'trainDataSet' )\n",
    "save_obj(hmmModels, 'hmmModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDir = \"Commands_Dataset/test_noisy/\"\n",
    "testDataSet = buildGoogleDataSet(testDir)\n",
    "hmmModels = load_obj('hmmModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = testDataSet['up'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "for label_test in testDataSet.keys():\n",
    "    for num in range(len(testDataSet[label_test])):\n",
    "        sample = testDataSet[label_test][num]\n",
    "        scoreList = {}\n",
    "        for model_label in hmmModels.keys():\n",
    "            model = hmmModels[model_label]\n",
    "            score = model.score(sample)\n",
    "            scoreList[model_label] = score\n",
    "        predict = max(scoreList, key=scoreList.get)\n",
    "        if predict == label_test:\n",
    "            correct_count = correct_count+1\n",
    "        total_count = total_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((10, 10))\n",
    "answers = list(testDataSet.keys())\n",
    "for idx,label_test in enumerate(testDataSet.keys()):\n",
    "    for num in range(len(testDataSet[label_test])):\n",
    "        sample = testDataSet[label_test][num]\n",
    "        scoreList = {}\n",
    "        for model_label in hmmModels.keys():\n",
    "            model = hmmModels[model_label]\n",
    "            score = model.score(sample)\n",
    "            scoreList[model_label] = score\n",
    "        predict = max(scoreList, key=scoreList.get)\n",
    "        confusion_matrix[idx][answers.index(predict)] = confusion_matrix[idx][answers.index(predict)] + 1\n",
    "        if predict == label_test:\n",
    "            correct_count = correct_count+1\n",
    "        total_count = total_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {}\".format((correct_count*1.0)/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix/total_count\n",
    "plt.matshow(confusion_matrix)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
